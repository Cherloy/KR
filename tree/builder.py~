import math
from collections import Counter, defaultdict
from tree.node import DecisionNode
from tree.splitter import split_numeric, split_categorical, generate_numeric_thresholds
from tree.metrics import entropy

def majority_class(labels):
    return Counter(labels).most_common(1)[0][0]

def gain_ratio_with_missing(parent_labels, groups, group_weights):
    total_weight = sum(group_weights)
    weighted_entropy = sum(
        (w / total_weight) * entropy(g)
        for g, w in zip(groups, group_weights)
        if w > 0
    )
    gain = entropy(parent_labels) - weighted_entropy

    # Split information
    split_info = -sum(
        (w / total_weight) * math.log2(w / total_weight)
        for w in group_weights
        if w > 0
    )
    if split_info == 0:
        return 0
    return gain / split_info

def choose_best_split(dataset, labels, attribute_types, verbose=True):
    best_ratio = -1
    best_attr = None
    best_threshold = None
    best_splits = None

    n = len(dataset)
    for i in range(len(dataset[0])):
        known_data, known_labels, unknown_data, unknown_labels = [], [], [], []
        for x, y in zip(dataset, labels):
            if x[i] is None or x[i] == '?':
                unknown_data.append(x)
                unknown_labels.append(y)
            else:
                known_data.append(x)
                known_labels.append(y)

        if len(known_data) == 0:
            continue

        attr_type = attribute_types[i]

        if attr_type == 'numerical':
            thresholds = generate_numeric_thresholds(known_data, i)
            for t in thresholds:
                (left_data, left_labels), (right_data, right_labels) = split_numeric(known_data, known_labels, i, t)
                if not left_labels or not right_labels:
                    continue

                left_w = len(left_labels)
                right_w = len(right_labels)
                total_known = left_w + right_w

                # Добавим пропущенные равномерно
                left_labels += unknown_labels
                right_labels += unknown_labels
                left_w += len(unknown_labels) * (left_w / total_known)
                right_w += len(unknown_labels) * (right_w / total_known)

                ratio = gain_ratio_with_missing(labels, [left_labels, right_labels], [left_w, right_w])

                if verbose:
                    print(f"[NUM] attr {i}, threshold {t:.3f}, gain ratio = {ratio:.4f}")

                if ratio > best_ratio:
                    best_ratio = ratio
                    best_attr = i
                    best_threshold = t
                    best_splits = (left_data, left_labels, right_data, right_labels)

        else:
            splits = split_categorical(known_data, known_labels, i)
            total_known = sum(len(labels) for _, labels in splits.values())
            group_labels = []
            group_weights = []
            for value, (sub_data, sub_labels) in splits.items():
                group_labels.append(sub_labels + unknown_labels)
                weight = len(sub_labels) + len(unknown_labels) * (len(sub_labels) / total_known)
                group_weights.append(weight)

            ratio = gain_ratio_with_missing(labels, group_labels, group_weights)

            if verbose:
                print(f"[CAT] attr {i}, values = {len(splits)}, gain ratio = {ratio:.4f}")

            if ratio > best_ratio:
                best_ratio = ratio
                best_attr = i
                best_threshold = None
                best_splits = splits

    if verbose:
        print(f"==> Selected attr {best_attr}, threshold {best_threshold}, gain ratio = {best_ratio:.4f}\n")

    return best_attr, best_threshold, best_splits

def build_tree(dataset, labels, attribute_types, min_samples_split=2, depth=0):
    if len(set(labels)) == 1:
        return DecisionNode(label=labels[0], is_leaf=True)

    if len(dataset) < min_samples_split:
        return DecisionNode(label=majority_class(labels), is_leaf=True)

    best_attr, best_threshold, best_splits = choose_best_split(dataset, labels, attribute_types)

    if best_attr is None:
        return DecisionNode(label=majority_class(labels), is_leaf=True)

    node = DecisionNode(attribute=best_attr, threshold=best_threshold)

    if attribute_types[best_attr] == 'numerical':
        left_data, left_labels, right_data, right_labels = best_splits
        node.branches['<='] = build_tree(left_data, left_labels, attribute_types, min_samples_split, depth + 1)
        node.branches['>'] = build_tree(right_data, right_labels, attribute_types, min_samples_split, depth + 1)
    else:
        for value, (sub_data, sub_labels) in best_splits.items():
            node.branches[value] = build_tree(sub_data, sub_labels, attribute_types, min_samples_split, depth + 1)

    return node
