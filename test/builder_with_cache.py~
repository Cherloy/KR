import math
from collections import Counter, defaultdict
from tree.node import DecisionNode
from tree.splitter import split_numeric, split_categorical, generate_numeric_thresholds
from tree.metrics import entropy

# Ручной кэш для generate_numeric_thresholds
_numeric_thresholds_cache = defaultdict(list)


def generate_numeric_thresholds_cached(known_data_tuple, i):
    # Проверяем, есть ли результат в кэше
    cache_key = (known_data_tuple, i)
    if cache_key in _numeric_thresholds_cache:
        return _numeric_thresholds_cache[cache_key]

    # Преобразуем кортеж кортежей обратно в список списков
    known_data = [list(x) for x in known_data_tuple]
    # Вычисляем пороги
    thresholds = generate_numeric_thresholds(known_data, i)
    # Сохраняем результат в кэш
    _numeric_thresholds_cache[cache_key] = thresholds
    return thresholds


def majority_class(labels):
    return Counter(labels).most_common(1)[0][0]


def majority_class_in_tree(node):
    if node is None:
        return None
    if node.is_leaf:
        return node.label
    labels = []
    for child in node.branches.values():
        label = majority_class_in_tree(child)
        if label is not None:
            labels.append(label)
    return Counter(labels).most_common(1)[0][0] if labels else None


def gain_ratio_with_missing(parent_labels, groups, group_weights):
    total_weight = sum(group_weights)
    weighted_entropy = sum(
        (w / total_weight) * entropy(g)
        for g, w in zip(groups, group_weights)
        if w > 0
    )
    gain = entropy(parent_labels) - weighted_entropy

    # Split information
    split_info = -sum(
        (w / total_weight) * math.log2(w / total_weight)
        for w in group_weights
        if w > 0
    )
    if split_info == 0:
        return 0
    return gain / split_info


def choose_best_split(dataset, labels, attribute_types, verbose=False):
    best_ratio = -1
    best_attr = None
    best_threshold = None
    best_splits = None

    n = len(dataset)
    for i in range(len(dataset[0])):
        known_data, known_labels, unknown_data, unknown_labels = [], [], [], []
        for x, y in zip(dataset, labels):
            if x[i] is None or x[i] == '?':
                unknown_data.append(x)
                unknown_labels.append(y)
            else:
                known_data.append(x)
                known_labels.append(y)

        if len(known_data) == 0:
            continue

        attr_type = attribute_types[i]

        if attr_type == 'numerical':
            # Преобразуем known_data в кортеж кортежей для хэширования
            known_data_tuple = tuple(tuple(x) for x in known_data)
            thresholds = generate_numeric_thresholds_cached(known_data_tuple, i)
            for t in thresholds:
                (left_data, left_labels), (right_data, right_labels) = split_numeric(known_data, known_labels, i, t)
                if not left_labels or not right_labels:
                    continue

                left_w = len(left_labels)
                right_w = len(right_labels)
                total_known = left_w + right_w

                # Добавим пропущенные равномерно
                left_labels += unknown_labels
                right_labels += unknown_labels
                left_w += len(unknown_labels) * (left_w / total_known)
                right_w += len(unknown_labels) * (right_w / total_known)

                ratio = gain_ratio_with_missing(labels, [left_labels, right_labels], [left_w, right_w])

                if verbose:
                    print(f"[NUM] attr {i}, threshold {t:.3f}, gain ratio = {ratio:.4f}")

                if ratio > best_ratio:
                    best_ratio = ratio
                    best_attr = i
                    best_threshold = t
                    best_splits = (left_data, left_labels, right_data, right_labels)

        else:
            splits = split_categorical(known_data, known_labels, i)
            total_known = sum(len(labels) for _, labels in splits.values())
            group_labels = []
            group_weights = []
            for value, (sub_data, sub_labels) in splits.items():
                group_labels.append(sub_labels + unknown_labels)
                weight = len(sub_labels) + len(unknown_labels) * (len(sub_labels) / total_known)
                group_weights.append(weight)

            ratio = gain_ratio_with_missing(labels, group_labels, group_weights)

            if verbose:
                print(f"[CAT] attr {i}, values = {len(splits)}, gain ratio = {ratio:.4f}")

            if ratio > best_ratio:
                best_ratio = ratio
                best_attr = i
                best_threshold = None
                best_splits = splits

    if verbose:
        print(f"==> Selected attr {best_attr}, threshold {best_threshold}, gain ratio = {best_ratio:.4f}\n")

    return best_attr, best_threshold, best_splits


def build_tree(dataset, labels, attribute_types, min_samples_split=2, depth=0):
    if len(set(labels)) == 1:
        return DecisionNode(label=labels[0], is_leaf=True)

    if len(dataset) < min_samples_split:
        return DecisionNode(label=majority_class(labels), is_leaf=True)

    best_attr, best_threshold, best_splits = choose_best_split(dataset, labels, attribute_types)

    if best_attr is None:
        return DecisionNode(label=majority_class(labels), is_leaf=True)

    node = DecisionNode(attribute=best_attr, threshold=best_threshold)

    if attribute_types[best_attr] == 'numerical':
        left_data, left_labels, right_data, right_labels = best_splits
        node.branches['<='] = build_tree(left_data, left_labels, attribute_types, min_samples_split, depth + 1)
        node.branches['>'] = build_tree(right_data, right_labels, attribute_types, min_samples_split, depth + 1)
    else:
        for value, (sub_data, sub_labels) in best_splits.items():
            node.branches[value] = build_tree(sub_data, sub_labels, attribute_types, min_samples_split, depth + 1)

    return node


def predict_single(node, x, attribute_types, default_class=0):
    if node.is_leaf:
        return node.label

    if attribute_types[node.attribute] == 'numerical':
        if x[node.attribute] <= node.threshold:
            next_node = node.branches['<=']
        else:
            next_node = node.branches['>']
    else:
        value = x[node.attribute]
        if value not in node.branches:
            majority = majority_class_in_tree(node)
            return majority if majority is not None else default_class
        next_node = node.branches[value]

    return predict_single(next_node, x, attribute_types, default_class)


def predict_batch(tree, dataset, attribute_types, default_class=0):
    predictions = []
    for x in dataset:
        pred = predict_single(tree, x, attribute_types, default_class)
        if pred is None:
            pred = default_class
        predictions.append(pred)
    return predictions